{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "direct-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "narrow-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worst-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = Path('/nfs_storage/fs-mnt6/vaibhavg/data/images/male/')\n",
    "assert images_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "certain-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = get_image_files(images_path, recurse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "plastic-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(elem):\n",
    "    return Path(elem)\n",
    "\n",
    "def get_y(row):\n",
    "    return [0.0 for _ in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rubber-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, RegressionBlock),\n",
    "    get_x = get_x,\n",
    "    get_y = get_y,\n",
    "    splitter=RandomSplitter(),\n",
    "    item_tfms=(Resize(224)),\n",
    "    batch_tfms=[\n",
    "        FlipItem(p=0.5),\n",
    "        Brightness(max_lighting=0.3, p=0.7, draw=None, batch=False),\n",
    "        Saturation(max_lighting=0.3, p=0.7, draw=None, batch=False),\n",
    "        Hue(max_hue=0.1, p=0.75, draw=None, batch=False),\n",
    "        RandomErasing(p=0.2, sl=0.0, sh=0.15, max_count=6, min_aspect=0.2)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "careful-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "neural-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = cnn_learner(dls, resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "solved-information",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((0.2641298770904541,\n",
       "  -1.9832581281661987,\n",
       "  1.3707842826843262,\n",
       "  0.9456096291542053,\n",
       "  -5.0808563232421875,\n",
       "  -4.248899936676025,\n",
       "  5.36594820022583,\n",
       "  4.451999664306641,\n",
       "  -0.2664620876312256,\n",
       "  -1.0456044673919678,\n",
       "  -1.9824739694595337,\n",
       "  -2.1637892723083496,\n",
       "  -1.431849718093872,\n",
       "  3.440964937210083,\n",
       "  2.5792007446289062),\n",
       " tensor([ 0.2641, -1.9833,  1.3708,  0.9456, -5.0809, -4.2489,  5.3659,  4.4520,\n",
       "         -0.2665, -1.0456, -1.9825, -2.1638, -1.4318,  3.4410,  2.5792]),\n",
       " tensor([ 0.2641, -1.9833,  1.3708,  0.9456, -5.0809, -4.2489,  5.3659,  4.4520,\n",
       "         -0.2665, -1.0456, -1.9825, -2.1638, -1.4318,  3.4410,  2.5792]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.predict(images_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ignored-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_path = Path('trp_wt=grp_wt=1_non_progressive_full_face.pkl')\n",
    "\n",
    "def get_x(row):\n",
    "    return Path(row['human_img_path'])\n",
    "\n",
    "def get_y(row):\n",
    "    return get_lips_params_avatar_imagename(row['winner']), \\\n",
    "           get_lips_params_avatar_imagename(row['loser']), \\\n",
    "           row['round_num']\n",
    "\n",
    "def loss(pred, label, trp_wt=1, group_loss_wt=1, point_loss_wt=0, MARGIN_TRIPLET=0.2):\n",
    "\n",
    "    def triplet_loss(pred, label, MARGIN=MARGIN_TRIPLET):\n",
    "        ''' Analysis can be seen in the notebook - loss_function '''\n",
    "        winner, loser, round_num = label\n",
    "\n",
    "        winner_dist = ((pred - winner) ** 2).mean(axis=1)\n",
    "        loser_dist = ((pred - loser) ** 2).mean(axis=1)\n",
    "\n",
    "        loss = winner_dist - loser_dist + MARGIN\n",
    "        return torch.relu(loss).mean()\n",
    "\n",
    "    def group_loss(pred, label):\n",
    "        winner, loser, round_num = label\n",
    "        return (nn.MSELoss()(pred, winner) + nn.MSELoss()(pred, loser)) / 2\n",
    "\n",
    "    def point_loss(pred, label, thresh = 2.9):\n",
    "        winner, loser, round_num = label\n",
    "        bs = winner.shape[0]\n",
    "\n",
    "        num_rounds_thresh = torch.where(round_num >= thresh, 1, 0).sum()\n",
    "        pt_loss_sum = torch.where(round_num > thresh, \n",
    "                          ((pred - winner) ** 2).mean(axis=1), \n",
    "                          torch.zeros((bs, )).float().to(device)).sum()\n",
    "\n",
    "        return torch.where( num_rounds_thresh > 0, pt_loss_sum / num_rounds_thresh, torch.tensor(0).float().to(device) )\n",
    "\n",
    "\n",
    "    return trp_wt * triplet_loss(pred, label) + \\\n",
    "           group_loss_wt * group_loss(pred, label) + \\\n",
    "           point_loss_wt * point_loss(pred, label)\n",
    "\n",
    "def splitter(df):\n",
    "    ''' We need to ensure that different images be used for training and validation, and not merely different\n",
    "        triplets for same images '''\n",
    "    TRAIN_SIZE = 0.8\n",
    "    VALID_SIZE = 1.0 - TRAIN_SIZE\n",
    "    train_idx = list( range(int(TRAIN_SIZE * len(df))) )\n",
    "    valid_idx = list( range(int(TRAIN_SIZE * len(df)), len(df)) )\n",
    "    return train_idx, valid_idx\n",
    "\n",
    "def mae_winner(pred, label):\n",
    "    winner, loser, round_num = label\n",
    "    return nn.L1Loss()(pred, winner)\n",
    "\n",
    "def mae_loser(pred, label):\n",
    "    winner, loser, round_num = label\n",
    "    return nn.L1Loss()(pred, loser)\n",
    "\n",
    "\n",
    "assert learner_path.exists()\n",
    "learner = load_learner(learner_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "focal-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.dls = dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "closed-savings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((-0.29914921522140503,\n",
       "  -0.07510409504175186,\n",
       "  -0.38681042194366455,\n",
       "  0.10719355940818787,\n",
       "  0.022745639085769653,\n",
       "  0.671205997467041,\n",
       "  0.14725103974342346,\n",
       "  0.10758504271507263,\n",
       "  0.19371020793914795,\n",
       "  -0.4709028899669647,\n",
       "  -0.23201623558998108,\n",
       "  -0.04340946674346924,\n",
       "  -0.10809613019227982,\n",
       "  0.4655865430831909,\n",
       "  0.09255486726760864),\n",
       " tensor([-0.2991, -0.0751, -0.3868,  0.1072,  0.0227,  0.6712,  0.1473,  0.1076,\n",
       "          0.1937, -0.4709, -0.2320, -0.0434, -0.1081,  0.4656,  0.0926]),\n",
       " tensor([-0.2991, -0.0751, -0.3868,  0.1072,  0.0227,  0.6712,  0.1473,  0.1076,\n",
       "          0.1937, -0.4709, -0.2320, -0.0434, -0.1081,  0.4656,  0.0926]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = learner.predict(images_list[0])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "third-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_learner_preds(preds):\n",
    "    mean = np.array([-2.285041, 19.212197, -16.371056, -2.285041, 19.212197, 16.371056, 1.150694, -1.504351, \n",
    "                     -4.001261, 0.323188, -19.492486, 0.000000, 0.000000, 9.950089, -19.391928])\n",
    "    std = np.array([7.615347, 8.641896, 17.199984, 7.615347, 8.641896, 17.199984, 9.583560, 5.826328, \n",
    "                    10.115759, 1.251700, 10.740775, 1.000000, 1.000000, 13.113880, 39.773768])\n",
    "    return np.array(preds) * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pressed-return",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.56316608,  18.56315522, -23.02418907,  -1.46872485,\n",
       "        19.40876245,  27.91578842,   2.56188317,  -0.87752525,\n",
       "        -2.04173522,  -0.26624115, -21.98452018,  -0.04340947,\n",
       "        -0.10809613,  16.05573506, -15.71067218])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unnormalize_learner_preds(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "powered-appointment",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function get_x at 0x7fa40f0fc5e0>: it's not the same object as __main__.get_x",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ddbf244bade0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trp_wt=grp_wt=1_non_progressive_full_face_pred_enabled.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nfs_storage/fs-mnt6/vaibhavg/conda3/envs/hikemoji/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, fname, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;31m#To avoid the warning that come from PyTorch about model not being checked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs_storage/fs-mnt6/vaibhavg/conda3/envs/hikemoji/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs_storage/fs-mnt6/vaibhavg/conda3/envs/hikemoji/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m     \u001b[0mdata_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function get_x at 0x7fa40f0fc5e0>: it's not the same object as __main__.get_x"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-mandate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
